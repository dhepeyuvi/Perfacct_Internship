{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nvidia_smi\n",
    "\n",
    "# Initialize NVML\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get GPU power draw\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        power_draw = (\n",
    "            nvidia_smi.nvmlDeviceGetPowerUsage(handle) / 1000.0\n",
    "        )  # Convert to watts\n",
    "\n",
    "        # Print power draw\n",
    "        print(\"Power Draw: {:.2f} W\".format(power_draw))\n",
    "\n",
    "        # Sleep for 1 second\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    # Shutdown NVML\n",
    "    nvidia_smi.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Extensions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Base Libraries\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from utility_scripts import utils\n",
    "import os\n",
    "\n",
    "# Utility py files\n",
    "from utility_scripts import tft_optimizer as tft\n",
    "\n",
    "# DL Base Libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use GPUS as is Required\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "import tensorflow.keras.applications as tf_app\n",
    "from tensorflow.keras.applications.mobilenet_v3 import (\n",
    "    preprocess_input,\n",
    "    decode_predictions,\n",
    ")\n",
    "\n",
    "# Model Conversion and Inference Libraries\n",
    "import torch\n",
    "import onnx\n",
    "import tf2onnx\n",
    "import onnxruntime as rt\n",
    "from onnx2torch import convert\n",
    "\n",
    "from EMA import (\n",
    "    EMA_finalize,\n",
    "    EMA_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = int(input(\"\\n\\nIf using a jnb enter 1 else 0\\n\"))\n",
    "uni = int(input(\"\\n\\nIf using uni gpu enter 1 else 0\\n\"))\n",
    "gpu_id = int(input(\"\\n\\n Which GPU to USE 1 else 0\\n\"))\n",
    "# If everything is run from jupyter nb then results file generated will have a suffix of uni\n",
    "\n",
    "results_suffix = \"uni\" if uni else \"work\"\n",
    "results_suffix += \"_nb\" if nb else \"_py\"\n",
    "print(f\"Suffix used with result files will be {results_suffix}!!\")\n",
    "\n",
    "\n",
    "GPU_ID = gpu_id\n",
    "print(f\"GPU: {GPU_ID} is being used\")\n",
    "\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model names and directories for saving\n",
    "model_name = \"MobileNetV3L\"\n",
    "results_directory = \"benchmark_results\"\n",
    "results_save_path = os.path.join(results_directory, model_name)\n",
    "\n",
    "if not os.path.exists(results_save_path):\n",
    "    print(f\"Results Dir {results_save_path} doesn't exist Creating!!\")\n",
    "    os.makedirs(results_save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "models_directory = \"models_lib\"\n",
    "model_type = \"tf_models\"\n",
    "tf_model_save_path = os.path.join(models_directory, model_type)\n",
    "if not os.path.exists(tf_model_save_path):\n",
    "    print(f\"Save Path {tf_model_save_path} doesn't exist Creating!!\")\n",
    "    os.makedirs(tf_model_save_path, exist_ok=True)\n",
    "\n",
    "# Creation of directory for trt_models\n",
    "models_directory = \"models_lib\"\n",
    "model_type = \"trt_models\"\n",
    "trt_model_save_path = os.path.join(models_directory, model_type)\n",
    "if not os.path.exists(trt_model_save_path):\n",
    "    print(f\"Save Path {trt_model_save_path} doesn't exist Creating!!\")\n",
    "    os.makedirs(trt_model_save_path, exist_ok=True)\n",
    "\n",
    "# Creation of directory for onnx models\n",
    "models_directory = \"models_lib\"\n",
    "model_type = \"onnx_models\"\n",
    "onnx_model_save_path = os.path.join(models_directory, model_type)\n",
    "if not os.path.exists(onnx_model_save_path):\n",
    "    print(f\"Save Path {onnx_model_save_path} doesn't exist Creating!!\")\n",
    "    os.makedirs(onnx_model_save_path, exist_ok=True)\n",
    "\n",
    "# Creation of directory for onnx models\n",
    "models_directory = \"models_lib\"\n",
    "model_type = \"torch_models\"\n",
    "torch_model_save_path = os.path.join(models_directory, model_type)\n",
    "if not os.path.exists(torch_model_save_path):\n",
    "    print(f\"Save Path {torch_model_save_path} doesn't exist Creating!!\")\n",
    "    os.makedirs(torch_model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "tf_model = tf_app.MobileNetV3Large(weights=\"imagenet\", include_top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Making Prediction with Downloaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Doing one prediction is necessary to compile the model\n",
    "# Loading n preprocessing the image\n",
    "# url = \"https://images.dog.ceo/breeds/retriever-golden/n02099601_3004.jpg\"\n",
    "url = \"https://i.pinimg.com/originals/56/ea/2b/56ea2bb991a7446776ac2f2f27fdc397.jpg\"\n",
    "\n",
    "img = resize(io.imread(url), (224, 224))\n",
    "img = 255 * np.expand_dims(img, axis=0)\n",
    "img = preprocess_input(img)\n",
    "preds = tf_model.predict(img)\n",
    "print(f\"Predicted {decode_predictions(preds, top = 3)[0]}\")\n",
    "plt.imshow(img[0] / 255)\n",
    "plt.title(decode_predictions(preds, top=3)[0][0][1])\n",
    "plt.axis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_model.save(os.path.join(tf_model_save_path, model_name))\n",
    "# tf_model.save(f\"{os.path.join(tf_model_save_path, model_name)}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = tf.keras.models.load_model(\n",
    "    os.path.join(tf_model_save_path, model_name)\n",
    ")\n",
    "keras_model = tf.keras.models.load_model(\n",
    "    f\"{os.path.join(tf_model_save_path, model_name)}.keras\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Converting & Benchmarking for TF-TRT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Inferencing & Benchmarking TF-TRT FP32 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting and saving the model\n",
    "# NOTE Load the Cuda and tensorrt Modules\n",
    "PRECISION = \"FP32\"\n",
    "print(\"Converting to TF-TRT FP32...\")\n",
    "file_name = f\"{model_name}_TFTRT_{PRECISION}\"\n",
    "trt_model_path = os.path.join(trt_model_save_path, file_name)\n",
    "\n",
    "opt_model = tft.ModelOptimizer(os.path.join(tf_model_save_path, model_name))\n",
    "trt_fp32 = opt_model.convert(trt_model_path, precision=PRECISION)\n",
    "print(f\"Done Converting to TF-TRT {PRECISION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# url = \"https://images.dog.ceo/breeds/retriever-golden/n02099601_3004.jpg\"\n",
    "url = \"https://i.pinimg.com/originals/56/ea/2b/56ea2bb991a7446776ac2f2f27fdc397.jpg\"\n",
    "img = io.imread(url)\n",
    "input_data, _ = utils.batch_sigle_img(\n",
    "    img,\n",
    "    target_size=(224, 224),\n",
    "    num_images=BATCH_SIZE,\n",
    "    preprocessor=preprocess_input,\n",
    ")\n",
    "input_data = input_data.astype(np.float32)\n",
    "\n",
    "# Load the saved model\n",
    "trt_fp32 = tft.OptimizedModel(trt_model_path)\n",
    "\n",
    "\n",
    "# preds = trt_fp32.predict(input_data).numpy()\n",
    "# print(f\"Predicted {decode_predictions(preds, top = 3)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencing the model\n",
    "EMA_init()\n",
    "num_warmup_runs = 50\n",
    "num_model_runs = 10\n",
    "fname = f\"TFTRT{PRECISION}_{num_model_runs}_it_{results_suffix}.csv\"\n",
    "csv_save_path = os.path.join(results_save_path, fname)\n",
    "\n",
    "utils.batch_model_performances(\n",
    "    framework_name=f\"TFTRT{PRECISION}\",\n",
    "    model=trt_fp32,\n",
    "    input_data=input_data,\n",
    "    batch_sizes=[8, 16, 32, 64, 128, 256, 512],\n",
    "    csv_path=csv_save_path,\n",
    "    num_warmup_runs=num_warmup_runs,\n",
    "    num_model_runs=num_model_runs,\n",
    "    trt=True,\n",
    "    onnx=False,\n",
    "    torch=False,\n",
    "    gpu_id=GPU_ID,\n",
    ")\n",
    "EMA_finalize()\n",
    "# 30m and 1.4m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Inferencing & Benchmarking TF-TRT FP16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting and saving the model\n",
    "PRECISION = \"FP16\"\n",
    "print(\"Converting to TF-TRT FP16...\")\n",
    "file_name = f\"{model_name}_TFTRT_{PRECISION}\"\n",
    "trt_model_path = os.path.join(trt_model_save_path, file_name)\n",
    "\n",
    "opt_model = tft.ModelOptimizer(os.path.join(tf_model_save_path, model_name))\n",
    "trt_fp16 = opt_model.convert(trt_model_path, precision=PRECISION)\n",
    "print(f\"Done Converting to TF-TRT {PRECISION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# url = \"https://images.dog.ceo/breeds/retriever-golden/n02099601_3004.jpg\"\n",
    "url = \"https://i.pinimg.com/originals/56/ea/2b/56ea2bb991a7446776ac2f2f27fdc397.jpg\"\n",
    "img = io.imread(url)\n",
    "input_data, _ = utils.batch_sigle_img(\n",
    "    img,\n",
    "    target_size=(224, 224),\n",
    "    num_images=BATCH_SIZE,\n",
    "    preprocessor=preprocess_input,\n",
    ")\n",
    "input_data = input_data.astype(np.float32)\n",
    "\n",
    "# Load the saved model\n",
    "trt_fp16 = tft.OptimizedModel(trt_model_path)\n",
    "\n",
    "\n",
    "# preds = trt_fp32.predict(input_data).numpy()\n",
    "# print(f\"Predicted {decode_predictions(preds, top = 3)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencing the model\n",
    "EMA_init()\n",
    "num_warmup_runs = 50\n",
    "num_model_runs = 10\n",
    "fname = f\"TFTRT{PRECISION}_{num_model_runs}_it_{results_suffix}.csv\"\n",
    "csv_save_path = os.path.join(results_save_path, fname)\n",
    "\n",
    "utils.batch_model_performances(\n",
    "    framework_name=f\"TFTRT{PRECISION}\",\n",
    "    model=trt_fp16,\n",
    "    input_data=input_data,\n",
    "    batch_sizes=[8, 16, 32, 64, 128, 256, 512],\n",
    "    csv_path=csv_save_path,\n",
    "    num_warmup_runs=num_warmup_runs,\n",
    "    num_model_runs=num_model_runs,\n",
    "    trt=True,\n",
    "    onnx=False,\n",
    "    torch=False,\n",
    "    gpu_id=GPU_ID,\n",
    ")\n",
    "\n",
    "EMA_finalize()\n",
    "# 7m and 1.4m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Inferencing & Benchmarking TF-TRT Int8 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you want to further reduce to INT8 precision depends on hardware - Turing cards and later INT8 is often better. Inference focused cards such as the NVIDIA T4 or systems-on-module such as Jetson AGX Xavier do well with INT8. In contrast, on a training-focused GPU like V100, INT8 often isn't any faster than FP16.\n",
    "\n",
    "To perform INT8 inference, we need to see what the normal range of activations are in the network so we can quantize our INT8 representations based on a normal set of values for our dataset. It is important that this dataset is representative of the testing samples in order to maintain accuracy levels.\n",
    "\n",
    "Here, we just want to see how our network performs in TensorRT from a runtime standpoint - so we will just feed dummy data and dummy calibration data into TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://images.dog.ceo/breeds/retriever-golden/n02099601_3004.jpg\"\n",
    "# img = io.imread(url)\n",
    "# int8_data = utils.batch_sigle_img(\n",
    "#     img, target_size=(224, 224), num_images=8, preprocessor=preprocess_input\n",
    "# )\n",
    "\n",
    "# Converting and saving the model\n",
    "# PRECISION = \"INT8\"\n",
    "# print(\"Converting to TF-TRT INT8...\")\n",
    "# save_dir = f\"models_lib/trt_models/{original_model_name}_TFTRT_{PRECISION}\"\n",
    "# opt_model = tft.ModelOptimizer(original_save_path)\n",
    "# opt_model.set_calibration_data(int8_data)\n",
    "# trt_int8  = opt_model.convert(save_dir, precision = PRECISION)\n",
    "# print(f\"Done Converting to TF-TRT {PRECISION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow TensorRT integration typically expects input data in the form of TensorFlow tensors. When working with TensorFlow models that are optimized or converted to run with TensorRT (e.g., using the trt.TrtGraphConverterV2), the inference is performed using TensorFlow tensor inputs.\n",
    "\n",
    "You can convert NumPy arrays to TensorFlow tensors using tf.constant or tf.convert_to_tensor before feeding them to a TensorFlow-TRT model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Converting & Benchmarking for Onnx models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Saving the tf model to onnx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape (replace this with the actual input shape of your model)\n",
    "input_shape = (None, 224, 224, 3)\n",
    "\n",
    "# Convert the TensorFlow model to ONNX format\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    tf_model,\n",
    "    input_signature=[\n",
    "        tf.TensorSpec(shape=input_shape, dtype=tf.float32, name=\"input\")\n",
    "    ],\n",
    ")\n",
    "onnx_model_path = os.path.join(onnx_model_save_path, model_name)\n",
    "# Save the ONNX model to a file\n",
    "with open(f\"{onnx_model_path}.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "# or\n",
    "# spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\n",
    "# output_path = \"models_lib/onnx_models/MobileNetV3L.onnx\"\n",
    "# model_proto, _ = tf2onnx.convert.from_keras(\n",
    "#     tf_model, input_signature=spec, opset=15, output_path=output_path\n",
    "# )\n",
    "# output_names = [n.name for n in model_proto.graph.output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = os.path.join(onnx_model_save_path, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Inferencing & Benchmarking Onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECISION = np.float32\n",
    "# url = \"https://images.dog.ceo/breeds/retriever-golden/n02099601_3004.jpg\"\n",
    "url = \"https://i.pinimg.com/originals/56/ea/2b/56ea2bb991a7446776ac2f2f27fdc397.jpg\"\n",
    "img = io.imread(url)\n",
    "input_data, _ = utils.batch_sigle_img(\n",
    "    img,\n",
    "    target_size=(224, 224),\n",
    "    num_images=BATCH_SIZE,\n",
    "    preprocessor=preprocess_input,\n",
    ")\n",
    "input_data = input_data.astype(PRECISION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the performance\n",
    "# providers = [\"CUDAExecutionProvider\"]\n",
    "# session = rt.InferenceSession(\n",
    "#     \"models_lib/onnx_models/MobileNetV3L.onnx\",\n",
    "#     providers=providers,\n",
    "# )\n",
    "# results = session.run([\"Predictions\"],{'input':input_data})\n",
    "# results = np.squeeze(results, axis=0)\n",
    "# print(f\"Predicted {decode_predictions((results),top=3)[0]}\")\n",
    "# # Y = io_binding.copy_outputs_to_cpu()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = rt.SessionOptions()\n",
    "# options.enable_profiling = True\n",
    "# providers = [\"CUDAExecutionProvider\"]\n",
    "# session = rt.InferenceSession(\n",
    "#     \"models_lib/onnx_models/MobileNetV3L.onnx\",\n",
    "#     providers=providers,\n",
    "#     sess_options=options,\n",
    "# )\n",
    "# io_binding = session.io_binding()\n",
    "# io_binding.bind_cpu_input(\"input\", input_data)\n",
    "# io_binding.bind_output(\"Predictions\")\n",
    "# session.run_with_iobinding(io_binding)\n",
    "# Y = io_binding.copy_outputs_to_cpu()[0]\n",
    "# print(f\"Predicted {decode_predictions(Y,top=3)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Inference\n",
    "EMA_init()\n",
    "\n",
    "providers = [\"CUDAExecutionProvider\"]\n",
    "session = rt.InferenceSession(f\"{onnx_model_path}.onnx\", providers=providers)\n",
    "num_warmup_runs = 50\n",
    "num_model_runs = 10\n",
    "batch_sizes = [8, 16, 32, 64, 128, 256, 512]\n",
    "fname = f\"onnxrt_{num_model_runs}_it_{results_suffix}.csv\"\n",
    "csv_save_path = os.path.join(results_save_path, fname)\n",
    "\n",
    "results = utils.batch_model_performances(\n",
    "    framework_name=\"onnxrt\",\n",
    "    model=session,\n",
    "    batch_sizes=batch_sizes,\n",
    "    num_warmup_runs=num_warmup_runs,\n",
    "    num_model_runs=num_model_runs,\n",
    "    input_data=input_data,\n",
    "    csv_path=csv_save_path,\n",
    "    onnx=True,\n",
    "    trt=False,\n",
    "    torch=False,\n",
    "    gpu_id=GPU_ID,\n",
    ")\n",
    "EMA_finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Converting and Benchmarking for .trt models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the onnx model\n",
    "# BATCH_SIZE = 32\n",
    "# PRECISION = np.float32\n",
    "# onnx_model_path = os.path.join(onnx_model_save_path, model_name)\n",
    "# onnx_model = onnx.load_model(f\"{onnx_model_path}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the inference BATCH_SIZE for infernce and perform inference NOTE with only this batch size nothing else.\n",
    "# inputs = onnx_model.graph.input\n",
    "# for input in inputs:\n",
    "#     dim1 = input.type.tensor_type.shape.dim[0]\n",
    "#     dim1.dim_value = BATCH_SIZE\n",
    "\n",
    "# # for input in onnx_model.graph.input:\n",
    "# #     for dim in input.type.tensor_type.shape.dim:\n",
    "# #         dim.dim_param = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx.save_model(\n",
    "#     onnx_model,\n",
    "#     os.path.join(onnx_model_save_path,model_name)+f\"_batch_size{BATCH_SIZE}.onnx\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Command to convert ONNX to handle dynamic input shape Via TRTEXEC (Doesn't work with min, opt, max, shapes options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "trtexec --onnx=models_lib/onnx_models/MobileNetV3L.onnx \\\n",
    "        --saveEngine=models_lib/trt_models/Mobilenet.trt \\\n",
    "        --explicitBatch \\\n",
    "        --minShapes=input_1:8x224x224x3 \\\n",
    "        --optShapes=input_1:32x224x224x3 \\\n",
    "        --maxShapes=input_1:512x224x224x3 \\\n",
    "        --shapes=input_1:16x224x224x3 \\\n",
    "        --workspace=1024*8<In MBs>\n",
    "```\n",
    "--explicitBatch: Specifies that the TensorRT engine should be optimized for varying batch sizes.\n",
    "\n",
    "--minShapes, --optShapes, --maxShapes: Define the range of batch sizes for which TensorRT should optimize the engine.\n",
    "\n",
    "--shapes: Specifies a preferred input shape for optimization, but it doesn't constrain the engine to only that shape. It's useful for indicating a common or preferred input size.\n",
    "\n",
    "--workspace: Sets the GPU workspace size for TensorRT optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_tftrt_dynamic_shapes/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "**Using Dynamic Shapes with TensorFlow TensorRT**\n",
    "\n",
    "The NVIDIA TensorRT is a library that facilitates high performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine which performs inference for that network. \n",
    "\n",
    "TensorFlow™ integration with TensorRT™ (TF-TRT) optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible.\n",
    "\n",
    "In this notebook demonstrates the use of dynamic shape tensors when using TensorFlow-TensorRT\n",
    "\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "If you are unfamiliar with how TensorFlow TensorRT works, you can refer to this [video](https://www.youtube.com/watch?v=w7871kMiAs8) for a quick overview. Some understanding of how TF-TRT works is required to digest the information in the following section. A quick and dirty explaination of the above is as follows: TF-TRT partitions the network graph into supported and unsupported sub-graphs. For each of these supported subgraphs, TRTEngineOp builds a TensorRT Engine. With this information in mind, let's proceed to the task at hand.\n",
    "\n",
    "TensorFlow TensorRT has two concepts relevent to this discussion:\n",
    "* Dynamic Ops\n",
    "* Dynamic Shape\n",
    "\n",
    "**Explaining Dynamic Ops**\n",
    "\n",
    "Dynamic Ops can be treated as a mode which let's users leverage the optimized model \"implicit shape\" mode, ie, if the model's input tensor shape is defined as(example) `[?, ?, ?, 3]`. How does this work? The TRTEngineOp creates the TensorRT engine at inference time with the shape of the input tensor (Let's say, `[8, 224, 224, 3]`). So up on execution, if we supply a tensor with a shape (say `[16, 224, 224, 3]`) another engine will be created. While this provides flexibility, the downside is that each TRT Engine consumes memory (a set of model weights for each \"profile\").\n",
    "\n",
    "###### Explaining Dynamic Shapes\n",
    "\n",
    "Dynamic Shape mode reqires the user to define, `minimum`, `optimial` and `maximum` shapes for the input tensor. This shifts the task at hand from being one about supporting implict tensor shape to supporting a set of explict batch shapes. The engine built in this case can handle any shape between the `minimum` and `maximum` shape, without a need for building separate engines.\n",
    "\n",
    "For a visual representation of the above, refer to the image below. The image on the right shows the scenerio where the use of three different shapes has resulted in three different engines as opposed to the one for dynamic shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Inferencing & Benchmarking .trt Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # url = \"https://images.dog.ceo/breeds/retriever-golden/n02099601_3004.jpg\"\n",
    "# url = \"https://i.pinimg.com/originals/56/ea/2b/56ea2bb991a7446776ac2f2f27fdc397.jpg\"\n",
    "# img = io.imread(url)\n",
    "# input_data = utils.batch_sigle_img(\n",
    "#     img,\n",
    "#     target_size=(224, 224),\n",
    "#     num_images=BATCH_SIZE,\n",
    "#     preprocessor=preprocess_input,\n",
    "# ).astype(PRECISION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Produce the trt file (Before running next cell)\n",
    "```bash\n",
    "polygraphy convert models_lib/onnx_models/MobileNetV3L_batch_size32.onnx --convert-to trt -o models_lib/trt_models/MobileNetV3L.trt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using the trtexec trt file\n",
    "# save_dir = \"models_lib/trt_models/MobileNetV3L.trt\"\n",
    "# trt_model = ONNXClassifierWrapper(\n",
    "#     save_dir,\n",
    "#     [BATCH_SIZE, 1000],\n",
    "#     target_dtype=PRECISION,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = trt_model.predict(input_data)\n",
    "# print(f\"Preds: {decode_predictions(preds, top = 3)[31]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Inferencing and Benchmarking models via pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://images.dog.ceo/breeds/retriever-golden/n02099601_3004.jpg\"\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "PRECISION = np.float32\n",
    "url = \"https://i.pinimg.com/originals/56/ea/2b/56ea2bb991a7446776ac2f2f27fdc397.jpg\"\n",
    "img = io.imread(url)\n",
    "input_data, _ = utils.batch_sigle_img(\n",
    "    img,\n",
    "    target_size=(224, 224),\n",
    "    num_images=BATCH_SIZE,\n",
    "    preprocessor=preprocess_input,\n",
    ")\n",
    "input_data = input_data.astype(PRECISION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to ONNX model\n",
    "onnx_model_path = os.path.join(onnx_model_save_path, model_name)\n",
    "# You can pass the path to the onnx model to convert it or...\n",
    "torch_model = convert(f\"{onnx_model_path}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the same device as the input data (GPU in this case)\n",
    "torch_model = torch_model.cuda()\n",
    "torch_model.eval()\n",
    "# Create example data on the GPU\n",
    "x = torch.tensor(input_data.transpose(0, 2, 1, 3), dtype=torch.float32).cuda()\n",
    "# preds = torch_model(x).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMA_init()\n",
    "num_warmup_runs = 50\n",
    "num_model_runs = 10\n",
    "fname = f\"torch_{num_model_runs}_it_{results_suffix}.csv\"\n",
    "csv_save_path = os.path.join(results_save_path, fname)\n",
    "batch_sizes = [8, 16, 32, 64, 128, 256]\n",
    "utils.batch_model_performances(\n",
    "    framework_name=\"torch\",\n",
    "    model=torch_model,\n",
    "    input_data=input_data,\n",
    "    batch_sizes=batch_sizes,\n",
    "    csv_path=csv_save_path,\n",
    "    num_warmup_runs=num_warmup_runs,\n",
    "    num_model_runs=num_model_runs,\n",
    "    trt=False,\n",
    "    torch=True,\n",
    "    gpu_id=GPU_ID,\n",
    ")\n",
    "EMA_finalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
